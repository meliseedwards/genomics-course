---
title: "Lab 11"
author: "Melise Edwards"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 11: Microbiome Analysis using dada2 and phyloseq

```{r}
BiocManager::install("dada2")
```

```{r}
BiocManager::install("phyloseq")
```

```{r}
BiocManager::install("DECIPHER")
```


```{r}
library("DECIPHER")
library("phyloseq")
library("dada2")
```


### Starting the [DADA2 Pipeline Tutorial](https://benjjneb.github.io/dada2/tutorial.html) which assumes that sequencing data:
* samples have been demultiplexed (e.g. split into individual per sample fastq files)
* non-biological nucleotides have been removed (e.g. primers, adapters, linkers, etc)
* if paired end sequencing, the forward and reverse fastq files contain reads in matched order


```{r}
path <- "data/MiSeq_SOP" 
list.files(path) # THIS IS SO COOL!!!! 
```
These fastq files were generated by 2x250 Illumina Miseq amplicon sequencing of the V4 region of the 16S rRNA gene from gut samples collected longitudinally from a mouse post-weaning. For now just consider them paired-end fastq files to be processed.


```{r}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq

fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```



## Inspect read quality profiles 

```{r}
# start by visualizing quality of the forward reads
plotQualityProfile(fnFs[1:2])

# grayscale = heatmap of frequency of each quality score at each base position
# green line = mean quality score 
# orange line = quality score distribution
# red line = scaled proportion of reads that extend to at least that position - a flat red line for Illumina since reads are usually the same length

# the forward reads are good quality; trimming required for the forward reads at position 240 (trimming the last 10 nucleotides)
```

```{r}
# now visualize the reverse reads 

plotQualityProfile(fnRs[1:2])

# grayscale = heatmap of frequency of each quality score at each base position
# green line = mean quality score 
# orange line = quality score distribution
# red line = scaled proportion of reads that extend to at least that position - a flat red line for Illumina since reads are usually the same length

# reverse reads significantly worse and especially at the end but apparently this is common in Illumina sequencing

# dada2 incorporates quality information intro its error model! trimming improves the algorithms sensitivity to rare variants though

# based on this, we will truncate or trim the reverse reads at position 160 
```
******

## Filter and Trim 


```{r}
#next, assign the filenames for the filtered fastq.gz files 

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


We’ll use standard filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE and maxEE=2. The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.

```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160), ## QUESTION - why wouldnt we use a vector of c(240, 240, 160, 160)
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=FALSE) # On Windows set multithread=FALSE
head(out)
```

##### NOTE: Alternatives: Zymo Research has recently developed a tool called Figaro that can help you choose DADA2 truncation length parameters: https://github.com/Zymo-Research/figaro#figaro

**** 

## Learn the Error Rates 

```{r}
errF <- learnErrors(filtFs, multithread=TRUE) 

# machine-learning, basically this 'learnErrors' method learns the error model for this data by alternating estimation of error rates and inference of sample composition until they converge on a jointly consistent solution; takes about 3 - 5 minutes.
```

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```

```{r}
# visualize the estimated error rates just as a sanity check

plotErrors(errF, nominalQ = TRUE)
```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence


******

## Sample Inference


```{r}
# applying the 'core sample inference algorithm' to the filtered and trimmed sequence data 

dadaFS <- dada(filtFs, err = errF, multithread = TRUE)
```

```{r}
# same for reverse 

dadaRs <- dada(filtRs, err = errR, multithread = TRUE)
```

```{r}
# inspecting returned dada-class object

dadaFS[[1]]
```

*****

## Merged paired reads 


```{r}
# now we merge the forward and reverse reads together to obtain the full clean or denoised sequences. To merge, we align the denoised forward reads with the denoised reverse reads and construct merged 'contiq' sequences

mergers <- mergePairs(dadaFS, filtFs, dadaRs, filtRs, verbose = TRUE)
```

```{r}
head(mergers[[1]])
```


******

## Construct sequence table


```{r}
# construct a amplicon sequence variant table (ASV table)
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

The sequence table is a matrix with **ROWS** corresponding to samples and **COLUMNS** corresponding to sequence variants. 

*****

## Remove chimeras 

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```

```{r}
sum(seqtab.nochim)/sum(seqtab) # here, chimeras make up 21% of the merged sequence variants, but when we account for the abundances of those variants they only account for about 4% of merged sequence reads. 
```

******

## Track reads through the pipeline 


```{r}
getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFS, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

*****

## Assign taxonomy 


```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```


```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

```{r}
library("DECIPHER"); packageVersion("DECIPHER")

dna <- DNAStringSet(getSequences(seqtab.nochim)) # Create a DNAStringSet from the ASVs

load("data/SILVA_SSU_r138_2019.RData")


ids <- IdTaxa(dna, trainingSet, strand="top", processors=NULL, verbose=FALSE) # use all processors

ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species") # ranks of interest

# Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy

taxid <- t(sapply(ids, function(x) {
        m <- match(ranks, x$rank)
        taxa <- x$taxon[m]
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))
colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)
```

*****

## Evaluate accuracy

```{r}
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
```
```{r}
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
```
*****

## Bonus: hand off to phyloseq 


```{r}
library(phyloseq)
library(Biostrings)
library(ggplot2)

theme_set(theme_bw())

```


```{r}
samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender <- substr(subject,1,1)
subject <- substr(subject,2,999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out
```


```{r}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
```


```{r}
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
```

```{r}
plot_richness(ps, x="Day", measures=c("Shannon", "Simpson"), color="When")
```


```{r}
# Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
```

```{r}
plot_ordination(ps.prop, ord.nmds.bray, color = "When", title = "Bray NMDS")
```


```{r}
# Bar plot - early/late differentiation

top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Day", fill="Family") + facet_wrap(~When, scales="free_x")
```

